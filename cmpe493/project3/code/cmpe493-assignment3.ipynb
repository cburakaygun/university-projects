{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMPE493 (Introduction to Information Retrieval) | Assignment 3: Cutting It Short\n",
    "### 2014400072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "**Model-1** constructs the vector representation of a sentence by taking the simple average of the vectors of all the words in that sentence.\n",
    "\n",
    "**Model-2** constructs the vector representation of a sentence by multiplying the vector of a word with that word's **tf-idf score** for all the words in that sentence and then taking the simple average of the results.\n",
    "\n",
    "Both models use the same tokenization mechanism for extracting words from a sentence. Tokenization includes removing some punctuation marks and lower-casing.\n",
    "\n",
    "After the sentences of an article are vectorized, both models run **k-means** algorithm with $k = \\sqrt{<length-of-article>}$ where `<length-of-article>` is the number of sentences in that article. Clusters are constructed with respect to **cosine similarity** and the most prominent sentence of each cluster is concatenated to form a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Parameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_PATH = '../glove.6B.200d.pkl'\n",
    "ARTICLES_PATH = '../articles/'\n",
    "GOLD_SUMMARIES_PATH = '../gold_summaries/'\n",
    "\n",
    "VECTOR_DIM = 200  # Dimension of GloVe vectors\n",
    "FOLD_K = 10  # K-Fold Cross-Validation parameter\n",
    "\n",
    "# Regex pattern used to replace some punctuation marks in article texts with space.\n",
    "PUNCTUATIONS_RGX = r'(\\\"|\\!|\\^|\\%|\\<|\\+|\\~|\\*|\\;|\\:|\\(|\\?|\\&|\\}|\\]|\\||\\,|\\'|\\)|\\-|\\#|\\`|\\@|\\/|\\$|\\_|\\{|\\.|\\>|\\[|\\\\|\\=)'\n",
    "\n",
    "# Regex pattern used to extract sentences from a paragraph.\n",
    "SENTENCE_SEPARATOR_RGX = r'''(?:(?<=[a-zA-Z0-9][.!?:])|(?<=[.!?:][\"']))\\s+(?=[a-zA-Z0-9\"'])'''\n",
    "\n",
    "\n",
    "ARTICLE_SENTENCES_DICT = {}  # Maps articles (filenames) to a List of sentences of that article.\n",
    "ARTICLE_GOLD_SUMMARY_DICT = {}  # Maps articles (filenames) to a String of gold summary of that article.\n",
    "ARTICLE_TF_DICT = {}  # Maps articles (filenames) to a Dictionary which maps terms in that article to their frequency.\n",
    "\n",
    "ROUGE = rouge.Rouge()\n",
    "\n",
    "with open(VECTOR_PATH, 'rb') as f:\n",
    "    GLOVE_DICT = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_by_sentences(article_text):\n",
    "    \"\"\"\n",
    "    Returns a List of sentences for given `article` (str). Article is assumed to contain one paragraph at a line.\n",
    "    \"\"\"\n",
    "    paragraphs = [line.strip() for line in re.split(r'(?:\\r?\\n)+', article_text) if len(line.strip()) != 0]\n",
    "\n",
    "    # Maps each paragraph to a List of sentences of that paragraph.\n",
    "    paragraph_sentences = [re.split(SENTENCE_SEPARATOR_RGX, paragraph) for paragraph in paragraphs]\n",
    "\n",
    "    return [sentence for sentence_list in paragraph_sentences\n",
    "            for sentence in sentence_list if re.search(r'[a-zA-Z0-9]', sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Returns a List of tokens (words) for given `sentence`. Tokenization includes removing some punctuation marks and\n",
    "    lower-casing.\n",
    "    \"\"\"\n",
    "    return [token.lower() for token in re.sub(PUNCTUATIONS_RGX, \" \", sentence).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, word_weight_dict=None):\n",
    "    \"\"\"\n",
    "    Returns the vector representation of the given `sentence`. The vector is calculated as the average\n",
    "    of the vectors of the words in `sentence`.\n",
    "\n",
    "    `word_weight_dict` is a Dictionary object which maps terms in `sentence` to a number (weight). If not None,\n",
    "    the vectors of the words are multiplied by the corresponding weights while calculating the average.\n",
    "    \"\"\"\n",
    "    # Returns a List of zeros (length VECTOR_DIM) if none of the words in `sentence` exists in GLOVE_DICT.\n",
    "    return [round(x, 5) for x in np.mean(\n",
    "        [np.array(GLOVE_DICT[word]) * (word_weight_dict.get(word, 0) if word_weight_dict else 1)\n",
    "         for word in tokenize(sentence) if word in GLOVE_DICT] or [np.zeros(VECTOR_DIM)], axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_of_vector(vector):\n",
    "    \"\"\"\n",
    "    Returns the length (magnitude) of given `vector`.\n",
    "    \"\"\"\n",
    "    return round(np.sqrt(sum([elem**2 for elem in vector])), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity of the given vectors `vec1` and `vec2`.\n",
    "    \"\"\"\n",
    "    dot_product = sum([e1*e2 for (e1, e2) in zip(vec1, vec2)])\n",
    "    return 0.0 if dot_product == 0 else round(dot_product/(length_of_vector(vec1) * length_of_vector(vec2)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(k, data_list):\n",
    "    \"\"\"\n",
    "    Separates given `data_list` into `k` clusters and returns a Tuple of (<cluster-centroids>, <data-clusters>).\n",
    "\n",
    "    Clusters are formed according to the cosine similarity and they are labeled as 0, 1, ..., k-1.\n",
    "    Initial centroids are selected as the random elements of `data_list` and at most 30 iterations are applied.\n",
    "\n",
    "    `k` is the number of clusters (int).\n",
    "    `data_list` is a List of instances; the instances can be scalar numbers or vectors (Lists of numbers).\n",
    "\n",
    "    <cluster-centroids> is a List of length `k`. ith element is the centroid of cluster-i.\n",
    "    <data-clusters> is a List, same length as `data_list`. ith element is the cluster (label) of the ith element\n",
    "    of `data_list`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Random data instances are chosen as the initial cluster centroids. ith element is the centroid of cluster-i.\n",
    "    unique_data_list = list(set(tuple(x) for x in data_list))\n",
    "    cluster_centroids = [unique_data_list[i] for i in np.random.choice(range(len(unique_data_list)), k, replace=False)]\n",
    "\n",
    "    # Stores the clusters to which the data instances are currently assigned.\n",
    "    data_clusters = []  # ith element is the cluster of the ith data instance in `data_list`.\n",
    "\n",
    "    step = 0  # Current iteration step.\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # Maps clusters to a List of data instances assigned to that cluster.\n",
    "        cluster_members_dict = {i: [] for i in range(k)}\n",
    "\n",
    "        # Stores the clusters to which the data instances are assigned after the re-assignment phase.\n",
    "        # ith element is the cluster of the ith data instance in `data_list`.\n",
    "        new_data_clusters = [-1 for _ in range(len(data_list))]\n",
    "\n",
    "        # Re-assignment of the data instances to the clusters.\n",
    "        for (i, data) in enumerate(data_list):\n",
    "            similarity_cluster_tuples = []  # List of (<cosine-similarity>, <cluster>) pairs\n",
    "\n",
    "            for (cluster, centroid) in enumerate(cluster_centroids):\n",
    "                similarity_cluster_tuples.append((cosine_similarity(data, centroid), cluster))\n",
    "\n",
    "            # Sorts by cosine similarity in descending order and takes the cluster with maximum cosine similarity.\n",
    "            chosen_cluster = sorted(similarity_cluster_tuples, reverse=True)[0][1]\n",
    "            cluster_members_dict[chosen_cluster].append(data)  # Data instance is assigned to the most similar centroid.\n",
    "            new_data_clusters[i] = chosen_cluster\n",
    "\n",
    "        if np.array_equal(new_data_clusters, data_clusters) or (step == 30):\n",
    "            break  # Stops if none of the data instances is re-assigned to a different cluster.\n",
    "\n",
    "        data_clusters = new_data_clusters\n",
    "\n",
    "        # Updates the centroids.\n",
    "        for cluster in cluster_members_dict:\n",
    "            if len(cluster_members_dict[cluster]) != 0:  # If some instance is assigned to this cluster, updates it.\n",
    "                cluster_centroids[cluster] = [round(x, 5) for x in np.mean(cluster_members_dict[cluster], axis=0)]\n",
    "\n",
    "    return cluster_centroids, data_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_summary(sentences, vectors, centroids, clusters):\n",
    "    \"\"\"\n",
    "    Forms and returns a summary for an article given as `sentences`. The most prominent sentence of each cluster is\n",
    "    taken and concatenated to form a summary.\n",
    "\n",
    "    :param sentences: A List of sentences of an article.\n",
    "    :param vectors: A List of vectors for sentences in `sentences`.\n",
    "    :param centroids: A List of centroid vectors. ith element is the centroid for cluster-i.\n",
    "    :param clusters: A List of cluster labels (int). ith element is the cluster of the ith sentence in `sentences`.\n",
    "    :return: Summary (str).\n",
    "    \"\"\"\n",
    "\n",
    "    # Maps clusters to a List of (<cosine-similarity>, <sentence-index>) pairs for sentences that belong that cluster.\n",
    "    # <cosine-similarity> is measured between a sentence and the centroid of the cluster that sentence belongs to.\n",
    "    cluster_sentences_dict = {i: [] for i in range(len(centroids))}\n",
    "\n",
    "    for (i, vector) in enumerate(vectors):\n",
    "        cluster = clusters[i]  # Cluster of the ith sentence/vector.\n",
    "        cluster_sentences_dict[cluster].append((cosine_similarity(centroids[cluster], vector), i))\n",
    "\n",
    "    summary = ''\n",
    "\n",
    "    for similarity_index_tuples in cluster_sentences_dict.values():\n",
    "        if similarity_index_tuples:  # If not empty List\n",
    "            # Sorts by cos-similarity in descending order and takes the sentence with the highest similarity.\n",
    "            summary += sentences[sorted(similarity_index_tuples, reverse=True)[0][1]]\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_summary(sentences):\n",
    "    \"\"\"\n",
    "    Constructs a summary for given article `sentences` via Model-1 and returns it. `sentences` is a List of sentences.\n",
    "    \"\"\"\n",
    "    vectors = [sentence_to_vector(s) for s in sentences]\n",
    "\n",
    "    # k_means() returns (<cluster-centroids>, <data-clusters>).\n",
    "    return form_summary(sentences, vectors, *k_means(int(round(np.sqrt(len(vectors)))), vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_summary(sentences, word_weight_dict):\n",
    "    \"\"\"\n",
    "    Constructs a summary for given article `sentences` via Model-2 and returns it. `sentences` is a List of sentences.\n",
    "    \"\"\"\n",
    "    vectors = [sentence_to_vector(s, word_weight_dict=word_weight_dict) for s in sentences]\n",
    "\n",
    "    # k_means() returns (<cluster-centroids>, <data-clusters>).\n",
    "    return form_summary(sentences, vectors, *k_means(int(round(np.sqrt(len(vectors)))), vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(train_articles, test_articles):\n",
    "    \"\"\"\n",
    "    A helper function for calculating Rouge scores of Model-1 and Model-2 on validation/test data set.\n",
    "\n",
    "    `train_articles` is a List of train articles (file names).\n",
    "    `test_articles` is a List of test articles (file names).\n",
    "\n",
    "    Returns a Tuple of (`model1_scores`, `model2_scores`)\n",
    "    \"\"\"\n",
    "    model1_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
    "    model2_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
    "\n",
    "    # TRAIN MODEL-2\n",
    "\n",
    "    term_df_dict = {}\n",
    "\n",
    "    for train_article in train_articles:\n",
    "        for term in ARTICLE_TF_DICT[train_article]:\n",
    "            if term not in term_df_dict:  # If `term` is encountered for the first time in this collection...\n",
    "                term_df_dict[term] = 1  # ... assigns a document-frequency of 1 to it.\n",
    "            else:\n",
    "                term_df_dict[term] += 1  # Else, increases its document-frequency by 1.\n",
    "\n",
    "    for test_article in test_articles:\n",
    "        # Maps terms (words) to their tf-idf values\n",
    "        word_weight_dict = {term: tf_idf_score(frequency, term_df_dict.get(term, 1), len(train_articles))\n",
    "                            for (term, frequency) in ARTICLE_TF_DICT[test_article].items()}\n",
    "\n",
    "        gold_summary = ARTICLE_GOLD_SUMMARY_DICT[test_article]\n",
    "\n",
    "        m1_summary = model1_summary(ARTICLE_SENTENCES_DICT[test_article])\n",
    "        m1_scores = ROUGE.get_scores(m1_summary, gold_summary)\n",
    "\n",
    "        for (rouge_type, scores) in m1_scores[0].items():\n",
    "            model1_scores[rouge_type].append(scores['f'])\n",
    "\n",
    "        m2_summary = model2_summary(ARTICLE_SENTENCES_DICT[test_article], word_weight_dict)\n",
    "        m2_scores = ROUGE.get_scores(m2_summary, gold_summary)\n",
    "\n",
    "        for (rouge_type, scores) in m2_scores[0].items():\n",
    "            model2_scores[rouge_type].append(scores['f'])\n",
    "\n",
    "    return model1_scores, model2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used to calculate TF-IDF score (`w`) of a term t in a document d is as follows:\n",
    "\n",
    "![tf-idf-weight.png](attachment:tf-idf-weight.png)\n",
    "\n",
    "The meanings of `tf`, `df` and `N` are explained in the function docstring below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score(tf, df, N):\n",
    "    \"\"\"\n",
    "    Calculates tf-idf score of a term t in a document d.\n",
    "    :param tf: Frequency of t in d (int, 0 <= tf)\n",
    "    :param df: Document frequency of t (int, 0 < df <= N)\n",
    "    :param N: Number of documents (int)\n",
    "    \"\"\"\n",
    "    return 0.0 if tf == 0 else round((1 + np.log10(tf)) * np.log10(N/df), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(256)  # Specifies a random seed to obtain the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below reads all the article and gold summary files and constructs `ARTICLE_GOLD_SUMMARY_DICT`, `ARTICLE_TF_DICT` and `ARTICLE_SENTENCES_DICT` dictionary objects.\n",
    "\n",
    "This block takes about 50sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article files... %100.00\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "articles = os.listdir(ARTICLES_PATH)\n",
    "\n",
    "print(\"Processing article files... %00.00\", end='')\n",
    "\n",
    "for (i, article) in enumerate(articles):\n",
    "    if (i+1) % 25 == 0:\n",
    "        for _ in range(5):\n",
    "            print(\"\\b\", end='', flush=True)\n",
    "        print(f\"{100*(i+1)/len(articles):05.2f}\", end='')\n",
    "    \n",
    "    with open(os.path.join(GOLD_SUMMARIES_PATH, article), errors='ignore') as f:  # Ignores encoding errors.\n",
    "        ARTICLE_GOLD_SUMMARY_DICT[article] = f.read()\n",
    "\n",
    "    with open(os.path.join(ARTICLES_PATH, article), errors='ignore') as f:  # Ignores encoding errors.\n",
    "        article_sentences = article_by_sentences(f.read())\n",
    "\n",
    "    ARTICLE_TF_DICT[article] = {}\n",
    "\n",
    "    for sentence in article_sentences:\n",
    "        for token in tokenize(sentence):\n",
    "            if token not in ARTICLE_TF_DICT[article]:  # If `token` is encountered for the first time in this article...\n",
    "                ARTICLE_TF_DICT[article][token] = 1  # ... assigns a term-frequency of 1 to it.\n",
    "            else:\n",
    "                ARTICLE_TF_DICT[article][token] += 1  # Else, increases its term-frequency by 1.\n",
    "\n",
    "    ARTICLE_SENTENCES_DICT[article] = article_sentences\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA SIZE = 1780\t\tTEST DATA SIZE = 445\n"
     ]
    }
   ],
   "source": [
    "# Separates articles randomly into TRAIN and TEST test with ratio 4:1.\n",
    "np.random.shuffle(articles)\n",
    "TRAIN_ARTICLES = articles[:int(len(articles)*0.8)]\n",
    "TEST_ARTICLES = articles[int(len(articles)*0.8):]\n",
    "\n",
    "\n",
    "print(f\"TRAIN DATA SIZE = {len(TRAIN_ARTICLES)}\\t\\tTEST DATA SIZE = {len(TEST_ARTICLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "This block takes about 6min30sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation is in progress... %100.00\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation is in progress... %00.00\", end='')\n",
    "\n",
    "m1_valid_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}  # Model-1 validation scores\n",
    "m2_valid_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}  # Model-2 validation scores\n",
    "\n",
    "fold_number = int(np.ceil(len(TRAIN_ARTICLES) / FOLD_K))\n",
    "\n",
    "for i in range(fold_number):\n",
    "    for _ in range(5):\n",
    "        print(\"\\b\", end='', flush=True)\n",
    "    print(f\"{100*(i+1)/fold_number:05.2f}\", end='')\n",
    "    \n",
    "    lower_bound = i * FOLD_K\n",
    "    upper_bound = lower_bound + FOLD_K\n",
    "\n",
    "    valid_articles = TRAIN_ARTICLES[lower_bound:upper_bound]  # Validation articles\n",
    "    other_articles = TRAIN_ARTICLES[:lower_bound] + TRAIN_ARTICLES[upper_bound:]\n",
    "\n",
    "    m1_scores, m2_scores = rouge_scores(other_articles, valid_articles)\n",
    "\n",
    "    for (rouge_type, score_list) in m1_scores.items():\n",
    "        m1_valid_scores[rouge_type].append(np.mean(score_list))\n",
    "\n",
    "    for (rouge_type, score_list) in m2_scores.items():\n",
    "        m2_valid_scores[rouge_type].append(np.mean(score_list))\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "This block takes about 1min30sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing is in progress... DONE.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing is in progress... \", end='')\n",
    "m1_test_scores, m2_test_scores = rouge_scores(TRAIN_ARTICLES, TEST_ARTICLES)\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMETRIC\t\tVALIDATION SCORE (mean +- std)\t\tTEST SCORE\n",
      "MODEL-1\n",
      "\t\trouge-1\t\t\t0.49502 +- 0.04185\t\t0.48264\n",
      "\t\trouge-2\t\t\t0.36418 +- 0.04979\t\t0.34789\n",
      "\t\trouge-l\t\t\t0.43774 +- 0.04422\t\t0.42616\n",
      "\n",
      "MODEL-2\n",
      "\t\trouge-1\t\t\t0.48537 +- 0.04704\t\t0.48616\n",
      "\t\trouge-2\t\t\t0.35295 +- 0.05529\t\t0.35576\n",
      "\t\trouge-l\t\t\t0.42286 +- 0.04956\t\t0.42512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\tMETRIC\\t\\tVALIDATION SCORE (mean +- std)\\t\\tTEST SCORE\")\n",
    "\n",
    "for (model, valid_scores, test_scores) in (('MODEL-1', m1_valid_scores, m1_test_scores), ('MODEL-2', m2_valid_scores, m2_test_scores)):\n",
    "    print(model)\n",
    "    for metric in ('rouge-1', 'rouge-2', 'rouge-l'):\n",
    "        print(f\"\\t\\t{metric}\"\n",
    "              f\"\\t\\t\\t{np.mean(valid_scores[metric]):.5f} +- {np.std(valid_scores[metric]):.5f}\"\n",
    "              f\"\\t\\t{np.mean(test_scores[metric]):.5f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The results (Rouge scores) above suggest that **Model-1** and **Model-2** perform very similarly. I'm actually surprised by this outcome. Model-1 takes simple average of word vectors to calculate a sentence's vector whereas Model-2 weights the word vectors by their **tf-idf scores** before taking an average. I expect Model-2 to perform better than Model-1 since it is a more complex and it takes into consideration the \"values\" (*tf-idf scores*) of the words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
